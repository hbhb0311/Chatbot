# ì´ì§„ íŒë‹¨ (Binary Classification) ê°œë…



# ì´ì§„ íŒë‹¨ì— í™œì„±í™”í•¨ìˆ˜ ë„ì… ì´ìœ 

Q1. ì´ì§„íŒë‹¨ì„ ì‰½ê²Œ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ ë­ê°€ ìˆì„ê¹Œ? ê°’ 2ê°œë¥¼ ì¶œë ¥í•´ì„œ ë¹„êµí•´ë³´ë©´ ë˜ì§€ ì•Šì„ê¹Œ?

A1. ì•ì„œ ìš°ë¦¬ê°€ ë°°ìš´ í¼ì…‰íŠ¸ë¡ ì˜ ì„ í˜• ì—°ì‚°ì€ í•˜ë‚˜ì˜ ê°’ë§Œ ì¶œë ¥í•´ì„œ 2ê°œì˜ ê°’ ì¶œë ¥í•  ìˆ˜ê°€ ì—†ì–´

Q2. ê·¸ëŸ¼ ì„ê³„ì¹˜ë¥¼ ì„¤ì •í•´ì„œ ì„ í˜• ì—°ì‚° ê²°ê³¼ê°€ ì´ ê°’ì„ ë„˜ëŠ”ì§€ì— ë”°ë¼ í•˜ë‚˜ì˜ ê°’ì„ ì„ íƒí•´ ì¶œë ¥í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ?

A2. ê·¸ ë°©ë²•ì€ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•´ â†’ ê·¸ëŸ¼ ì—­ì „íŒŒ ê³¼ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜, í¸í–¥) ê°±ì‹ ì´ ì•ˆë˜ê² ì§€? 

Q3. ê·¸ëŸ¼ ì‹ ê²½ë§ ì—°ì‚° ê²°ê³¼ë¡œ í™•ë¥ ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì¶”ì •í•˜ê³ , 

ì´ ê°’ì´ 1ì— ê°€ê¹Œìš°ë©´ ì°¸, 0ì— ê°€ê¹Œìš°ë©´ ê±°ì§“ìœ¼ë¡œ ì„ íƒí•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ?

A3. ì„ í˜• ì—°ì‚° ê²°ê³¼ê°’ ìì²´ëŠ” (-inf, inf) ë¼ì„œ  0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì œí•œí•  ìˆ˜ê°€ ì—†ì–´

â‡’ ê²°êµ­! 

ì„ í˜• ì—°ì‚°ì—ì„œëŠ” ë²”ìœ„ì˜ ì œí•œì´ ì—†ëŠ” ì‹¤ìˆ«ê°’ ê³„ì‚° + ì´í›„ `ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜` ì´ìš©

ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹¤ìˆ«ê°’ì„ 0~1 ì‚¬ì´ì˜ ê°’(í™•ë¥ ê°’ì˜ ì„±ì§ˆ)ì„ ê°–ë„ë¡ ë³€í™˜!

ê·¸ë ‡ë‹¤ë©´ ì´ì§„ íŒë‹¨ì—ì„œëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ â†’ ì‹œê·¸ëª¨ì´ë“œ ì‚¬ìš©!

ê·¸ë ‡ë‹¤ë©´ íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•  ì†ì‹¤í•¨ìˆ˜ëŠ”? â†’ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©!

EX) íšŒê·€ë¶„ì„ì—ì„œëŠ” MSE(í‰ê· ì œê³±ì˜¤ì°¨) ì‚¬ìš©

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ :  $\sigma(x)$

: ì´ì§„ íŒë‹¨ì—ì„œ ì‚¬ìš©í•˜ëŠ” í™œì„±í™”í•¨ìˆ˜

## logit = log + odds

## Odds

: **ì„±ê³µí™•ë¥ ì´ ì‹¤íŒ¨í™•ë¥ ì— ë¹„í•´ ì–¼ë§ˆë‚˜ ë” í°ì§€**ë¥¼ ì„¤ëª…

ì¦‰, oddsê°€ í´ìˆ˜ë¡ ì„±ê³µí™•ë¥ ì´ í¬ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

![odds](./img/Untitled.png)

EX) ê²½ë§ˆì—ì„œ íŠ¹ì • ë§ì´ ì´ê¸¸ í™•ë¥ ì´ 75%ë¼ê³  í•œë‹¤ë©´,

odds = 0.75 / 0.25 = 3 

â†’ ì •ìˆ˜ í‘œí˜„ì„ ì´ìš©í•œë‹¤ë©´ 3:1ê³¼ ê°™ì´ í‘œí˜„ê°€ëŠ¥, ì¦‰ 4ê²Œì„ ì¤‘ì— 3ê²Œì„ì€ ì´ê¸°ê³  1ê²Œì„ì€ ì§„ë‹¤ëŠ” ì˜ë¯¸

## Logit

logitì˜ ì¥ì  â†’ ë„“ì€ ë²”ìœ„ì˜ ê°’ì„ ê°„ë‹¨í•˜ê²Œ í‘œí˜„ ê°€ëŠ¥, ë³€í™”ëŸ‰ë³´ë‹¤ëŠ” ë¹„ìœ¨ ê´€ì ì—ì„œ ë” ë¯¼ê°í•˜ê²Œ í¬ì°© ê°€ëŠ¥

![Logit](./img/Untitled1.png)

â†’ ë¡œì§“í•¨ìˆ˜ì™€ ì‹œê·¸ëª¨ì´ë“œí•¨ìˆ˜ëŠ” `ì—­í•¨ìˆ˜` ê´€ê³„

[ë¡œì§“ê³¼ ì‹œê·¸ëª¨ì´ë“œ ê°œë…ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…](https://chacha95.github.io/2019-04-04-logit/)

# êµì°¨ ì—”íŠ¸ë¡œí”¼

: ì´ì§„ íŒë‹¨ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜

## ì—”íŠ¸ë¡œí”¼

[[ë”¥ ëŸ¬ë‹] 4ê°•. ì •ë³´ì´ë¡  ê¸°ì´ˆ, Entropy & Cross-entropy & KL-divergence](https://www.youtube.com/watch?v=z1k8HVU4Mxc)

ë‹¨ìœ„ ì •ë³´ ìˆ˜ ^ ì •ë³´ëŸ‰ = í‘œí˜„ ë²”ìœ„

ì¦‰, ì „êµ¬ 4ê°œë¡œ í‘œí˜„í•œë‹¤ë©´ $2^4$ = 16 

â†’ $4 = log_2 16$ â‡’ ì •ë³´ëŸ‰ = log_ë‹¨ìœ„ ì •ë³´ìˆ˜ (í‘œí˜„ë²”ìœ„)

$H(x)=âˆ’âˆ‘p(x_i)log_2p(x_i)$

â†’ ë¡œê·¸ì˜ ë°‘ì´ 2ì¸ ê²½ìš° ë‹¨ìœ„ë¥¼ ì„€ë…„ ë˜ëŠ” ë¹„íŠ¸ë¼ê³  í•¨ 

$H(x)=âˆ’âˆ‘p(x_i)logp(x_i)$

â†’ ë°‘ì´ ìì—°ìƒìˆ˜ì¸ ê²½ìš°ëŠ” ë‚´íŠ¸ë¼ê³  í•¨

â‡’ **ì •ë³´ ì—”íŠ¸ë¡œí”¼**: ì •ë³´ëŸ‰ì˜ í‰ê· (ê¸°ëŒ“ê°’), ì–´ë–¤ í™•ë¥  ë¶„í¬ë¡œ ì¼ì–´ë‚˜ëŠ” ì‚¬ê±´ì„ í‘œí˜„í•˜ëŠ”ë° `í•„ìš”í•œ ì •ë³´ëŸ‰` 

ë”°ë¼ì„œ ê°’ì´ ğŸ” â†’ í•„ìš”í•œ ì •ë³´ëŸ‰ ğŸ” â†’ í™•ë¥ ë¶„í¬ì˜ ë¶ˆí™•ì‹¤ì„± ğŸ”

$H(x)=âˆ’âˆ‘p(x_i)logp(x_i)$ : ì •ë³´ëŸ‰ì˜ ê°€ì¤‘í‰ê· 

ë”°ë¼ì„œ,

$p(x_i)$ â‡’ ê°€ì¤‘í‰ê·  ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” í™•ë¥ ë¶„í¬(í•˜ë‚˜ì˜ ì •ë³´ëŸ‰ì´ ë‚˜ì˜¬ í™•ë¥ ?)

$âˆ’logp(x_i)$ â‡’ ì •ë³´ëŸ‰ì„ ì œê³µí•˜ëŠ” í™•ë¥ ë¶„í¬( $p(x_i)$ ì˜ ì •ë³´ëŸ‰)

## êµì°¨ ì—”íŠ¸ë¡œí”¼

$H(P, Q)=âˆ’âˆ‘p(x_i)logq(x_i)$

$logp(x_i)$  â¡ï¸   $logq(x_i)$ ë¡œ ë°”ê¾¸ì–´ì¤Œ

- $-logp(x_i)$ : í•˜ë‚˜ì˜ $p(x_i)$ í•­ì— ëŒ€í•œ ì •ë³´ëŸ‰

    â‡’  $-logq(x_i)$ [ì¦‰, **ë”¥ëŸ¬ë‹ì´ ì˜ˆì¸¡í•œ ì •ë³´ëŸ‰?**]ìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ë©´, 

    $-logq(xi)$ : í•˜ë‚˜ì˜ $q(x_i)$ í•­ì— ëŒ€í•œ ì •ë³´ëŸ‰ = í‹€ë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘

$H(x)=âˆ’âˆ‘p(x_i)logq(x_i)$ â†’ **í‹€ë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ì˜ í‰ê· **

ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ƒ,ğ‘„) â‰¥ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ƒ,ğ‘ƒ )

êµì°¨ ì—”íŠ¸ë¡œí”¼ì˜ ë²”ìœ„ëŠ” ë¬´í•œëŒ€ê¹Œì§€ ê°€ëŠ¥í•˜ì§€ë§Œ, í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ `ì—”íŠ¸ë¡œí”¼ ê°’ì— ìˆ˜ë ´`í•˜ëŠ” ê²ƒì„ ëª©ì 

êµì°¨ ì—”íŠ¸ë¡œí”¼ì— ëŒ€í•œ ì„¤ëª… + ì˜ˆì‹œ :  [https://gnoej671.tistory.com/26](https://gnoej671.tistory.com/26)

ì—”íŠ¸ë¡œí”¼ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª… :  [http://melonicedlatte.com/machinelearning/2019/12/20/204900.html](http://melonicedlatte.com/machinelearning/2019/12/20/204900.html)

ì¶”í›„ì— ì°¸ê³ í•  ì‚¬ì´íŠ¸ (ë¡œì§“, ì‹œê·¸ëª¨ì´ë“œ, ì†Œí”„íŠ¸ë§¥ìŠ¤) :

[https://opentutorials.org/module/3653/22995](https://opentutorials.org/module/3653/22995)

# ì‹œê·¸ëª¨ì´ë“œ & êµì°¨ ì—”íŠ¸ë¡œí”¼

ì´ì§„ íŒë‹¨ ë¬¸ì œì— ëŒ€í•œ ì •ë‹µìœ¼ë¡œ $z$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ($z$ëŠ” 0 ë˜ëŠ” 1 ê°€ëŠ¥)

ë°ì´í„°ì˜ ê²°ê³¼ê°€ *ì°¸ì¼ í™•ë¥ * :  $P_T =z$ 

ë°ì´í„°ì˜ ê²°ê³¼ê°€ *ê±°ì§“ì¼ í™•ë¥ * : $P_F = 1-z$

---

**ì‹ ê²½ë§ ì˜ˆì¸¡ì— ëŒ€í•œ í™•ë¥ ê°’**ì€ (ì´ì§„ íŒë‹¨ ë¬¸ì œì—ì„œ í™œì„±í™”í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë‹ˆê¹Œ?)

$Q_T =\sigma(x)$ ,  $Q_F = 1-\sigma(x)$

---

â‡’ êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ : 

$H(P, Q)=âˆ’âˆ‘p(x_i)logq(x_i)$ $= -zlog\sigma(x) -(1-z)log(1-\sigma(x))$

![odds](./img/Untitled2.png)

ìœ„ì˜ ì‹ì„ ëŒ€ì…í•˜ë©´

â†’ 

![odds](./img/Untitled3.png)

â†’ ì´ì§„ íŒë‹¨ì—ì„œëŠ” z = 0 or z = 1

![odds](./img/Untitled4.png)

â†’ Hì˜ í¸ë¯¸ë¶„

![odds](./img/Untitled5.png)

[+) ì‹œê·¸ëª¨ì´ë“œ ë¯¸ë¶„](https://en.m.wikipedia.org/wiki/Logistic_function#Derivative)

# ì˜¤ë²„í”Œë¡œìš° ë¬¸ì œ í•´ê²°

## ì‹œê·¸ëª¨ì´ë“œ

```python
import numpy as np

def sigmoid(x):
  y = 1.0 / (1.0 + np.exp(-x))
  return y

print(sigmoid(-1000))
# ìŒìˆ˜ê°’ ë“¤ì–´ê°”ì„ ë•Œ overflow ê²½ê³ ë¬¸ ë°œìƒ
```

```
-> ì¶œë ¥ê°’ (overflow ê²½ê³ ë¬¸ ë°œìƒ)
0.0
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp
  after removing the cwd from sys.path.
```

```python
print(sigmoid(1000))
# -> ì¶œë ¥ê°’ : 1.0 (ì •ìƒì ìœ¼ë¡œ ë‚˜ì˜´)
```

â†’  ì‹ ë³€ê²½

![odds](./img/Untitled6.png)

```python
def sigmoid(x):
  y = np.exp(x) / (1.0 + np.exp(x))
  return y

sigmoid(-1000)
# ì¶œë ¥ê°’ -> 0.0 (ì •ìƒì ìœ¼ë¡œ ë‚˜ì˜´)
```

## êµì°¨ì—”íŠ¸ë¡œí”¼

```python
def crossentropy(x):
  return np.log(1+np.exp(-x))

crossentropy(-1000)
# ì¶œë ¥ê°’ -> inf(ê²½ê³ ë¬¸ ë°œìƒ)
# /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp
```

â†’ ì‹ ë³€ê²½

![odds](./img/Untitled7.png)

```python
def crossentropy(x):
  return -x + np.log(np.exp(x) + 1)

crossentropy(-1000)
# ì¶œë ¥ê°’ : 1000.0(ì •ìƒì ìœ¼ë¡œ ë‚˜ì˜´)
```

í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ì‹ ë³€í˜•ì€ xê°€ ìŒìˆ˜ì¼ë•Œë§Œ í•´ì¤˜ì•¼í•¨. ì–‘ìˆ˜ì¼ë•ŒëŠ” ë³¸ë˜ì˜ ì‹ì„ ì‚¬ìš© 

![odds](./img/Untitled8.png)

â†’ ì²˜ë¦¬ ê³¼ì •ì´ ë³µì¡í•´ ì•„ë˜ì˜ ì‹ìœ¼ë¡œ ë³€ê²½í•´ì¤Œ 

â‡’ max í•¨ìˆ˜ì™€ ì ˆëŒ€ê°’ì„ ì´ìš©í•´ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤.

![odds](./img/Untitled9.png)

